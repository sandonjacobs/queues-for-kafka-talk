= Kafka Shared Consumer Groups (KIP-932: Queues for Kafka)
Sandon Jacobs <sandon.jacobs@confluent.io>
:doctype: book
:icons: font
:source-highlighter: rouge
:toc: left
:toc-title: Table of Contents
:toclevels: 2
:sectnums:
:sectnumlevels: 2
:experimental:
:docinfo1:

== Overview

This project demonstrates the use of Apache Kafka's Share Consumers feature (KIP-932) through a practical example. It includes a producer that generates events and a consumer that processes them using the share consumer protocol.

== Project Structure

The project is organized into these modules:

* `kafka-producer`: Event producer application
* `kafka-queues-consumer`: Consumer application using share consumers

== Building and Testing

The project uses Maven for building and testing. A Makefile is provided to simplify common operations.

=== Building the Project

To build the entire project:

[source,bash]
----
make build
----

This will clean, compile, and package all modules.

=== Running Tests

To run all tests across all modules:

[source,bash]
----
make test
----

To run tests for specific modules:

[source,bash]
----
# Run tests for producer module
make test-kafka-producer

# Run tests for consumer module
make test-queue-consumer
----

== Docker Environment

The project includes a Docker Compose setup for running Kafka and related services.

=== Starting the Environment

To start the Kafka environment:

[source,bash]
----
make docker-start
----

This will start all required services in detached mode, including:
* Kafka broker
* Topic creation service (creates the `events-string` topic with 3 partitions)

=== Viewing Logs

To view logs from a specific service:

[source,bash]
----
# View all Docker logs
make docker-logs

# View Kafka logs
make docker-logs-kafka

# View topic creation logs
make docker-logs-topics
----

=== Stopping and Cleaning Up

To stop the environment:

[source,bash]
----
make docker-stop
----

To remove all containers and networks:

[source,bash]
----
make docker-remove
----

To stop and remove everything in one command:

[source,bash]
----
make docker-clean
----

== Running the Applications

=== Producer Application

To view help for the producer application:

[source,bash]
----
make help-kafka-producer
----

To run the producer with default configuration (runs in background):

[source,bash]
----
make run-kafka-producer
----

To run the producer with a custom properties file:

[source,bash]
----
make run-kafka-producer PRODUCER_PROPS=/path/to/properties
----

To run the producer with custom arguments:

[source,bash]
----
make run-kafka-producer ARGS="--duration 120 --interval 1000"
----

To view producer logs in real-time:

[source,bash]
----
make tail-producer-logs
----

To stop the producer:

[source,bash]
----
pkill -f ProducerApp
----

Available arguments:
* `--properties`: Path to Kafka client properties file (optional, defaults to classpath resource)
* `--duration`: Duration in seconds for the producer to run (default: 60)
* `--interval`: Interval in milliseconds between producing events (default: 500)

**Note**: The producer runs in the background and logs are written to `producer.log`. Use `make tail-producer-logs` to follow the logs in real-time.

=== Queue Consumer Application

To view help for the consumer application:

[source,bash]
----
make help-queue-consumer
----

To run the queue consumer with default configuration (runs in background with 10 consumers):

[source,bash]
----
make run-queue-consumer
----

To run the consumer with a custom properties file:

[source,bash]
----
make run-queue-consumer CONSUMER_PROPS=/path/to/properties
----

To run the consumer with custom arguments:

[source,bash]
----
make run-queue-consumer ARGS="--consumers 5"
----

To view consumer logs in real-time:

[source,bash]
----
make tail-consumer-logs
----

To stop the consumer:

[source,bash]
----
pkill -f ConsumerApp
----

Available arguments:
* `--properties`: Path to Kafka client properties file (optional, defaults to classpath resource)
* `--consumers`: Number of consumer instances to run (default: 10)

**Note**: The consumer runs in the background and logs are written to `consumer.log`. Use `make tail-consumer-logs` to follow the logs in real-time.

=== Log Files

Both applications run in the background and write their output to log files:

* `producer.log`: Contains all output from the producer application
* `consumer.log`: Contains all output from the consumer application

These log files are automatically created when you run the applications and are ignored by git. You can:

* View logs in real-time: `make tail-producer-logs` or `make tail-consumer-logs`
* View the entire log file: `cat producer.log` or `cat consumer.log`
* Clear logs: `rm producer.log consumer.log`

=== Running Both Applications

You can run both the producer and consumer simultaneously:

[source,bash]
----
# Start both applications in background
make run-kafka-producer
make run-queue-consumer

# View logs from both applications (in separate terminals)
make tail-producer-logs
make tail-consumer-logs

# Stop both applications
pkill -f ProducerApp
pkill -f ConsumerApp
----

== Getting Help

To view all available Makefile targets:

[source,bash]
----
make help
----

== License

This project is licensed under the Apache License 2.0.

== Acknowledgements

* Apache Kafka
* Confluent Platform
* Docker
